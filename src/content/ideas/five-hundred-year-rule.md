---
title: "The Five Hundred Year Rule"
description: "Why institutions, populations, and AI models all collapse in similar ways—and what relational knowledge has to do with it."
date: 2024-01-10
cover: /images/ideas/five-hundred-year.jpg
tags: [compression, institutions, AI, genetics]
status: mature
related:
  - what-gets-dropped
  - animate-grammar-ai
---

There's a pattern I keep seeing across wildly different domains: **things that work for about five hundred years, then stop working.**

## The pattern

Consider:
- **Institutions** that maintain coherent culture for ~500 years before fragmenting
- **Isolated populations** that maintain genetic diversity for ~500 years before inbreeding effects become critical
- **AI models** trained on outputs of previous models that degrade after ~5 generations

What connects these? They're all systems that *lose relational information over time*.

## The compression problem

Every time information passes through a bottleneck—whether that's institutional succession, reproductive isolation, or model training—something gets dropped. And what gets dropped is almost always the *relationships* between pieces of information, not the pieces themselves.

You can preserve facts. You can preserve rules. But the *why* behind them—the contextual understanding of how things connect—that's the first casualty.

> The map survives. The territory it described is forgotten.

## What this means for AI

Model collapse isn't just a technical problem. It's the same pattern that's been destroying institutional knowledge for millennia. When you train on outputs rather than originals, you're doing to information what inbreeding does to genes: preserving the structure while losing the diversity that made it robust.

## The Indigenous exception

Some knowledge systems have maintained coherence for far longer than 500 years. What do they do differently?

They encode relationships explicitly. Animate grammar. Verb-heavy languages. Stories that preserve *how* things connect rather than just *what* they are.

This might be the key to building AI systems that don't collapse.

---

*This is part of the larger thesis in [What Gets Dropped](/ideas/what-gets-dropped).*
